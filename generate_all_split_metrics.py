#!/usr/bin/env python3
"""
GENERATE ALL SPLIT METRICS - Main execution script

This script generates comprehensive performance metrics and visualizations
for each of the 4 dataset splits.
"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent / 'src'))

import logging
from split_metrics_generator import SplitMetricsGenerator

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def main():
    """Main execution."""
    
    print("\n" + "=" * 80)
    print("  GENERATING PERFORMANCE METRICS FOR ALL DATASET SPLITS")
    print("=" * 80 + "\n")
    
    # Create generator
    generator = SplitMetricsGenerator(base_output_dir='runs/detect/splits_metrics')
    
    # Generate all metrics (including new 80_10_10 split)
    generator.generate_all_metrics()
    
    print("\n" + "=" * 80)
    print("  âœ… ALL PERFORMANCE METRICS GENERATED SUCCESSFULLY!")
    print("=" * 80)
    
    print("\nðŸ“Š GENERATED FILES STRUCTURE:\n")
    print("  runs/detect/splits_metrics/")
    print("  â”œâ”€â”€ 82.7_17.3/")
    print("  â”‚   â”œâ”€â”€ training/")
    print("  â”‚   â”‚   â””â”€â”€ training_curves.png       (F1, Accuracy, Precision, Recall vs Epochs)")
    print("  â”‚   â””â”€â”€ metrics/")
    print("  â”‚       â”œâ”€â”€ confusion_matrix.png      (Confusion Matrix)")
    print("  â”‚       â”œâ”€â”€ metrics_summary.png       (All metrics overview)")
    print("  â”‚       â”œâ”€â”€ roc_auc_curve.png         (ROC-AUC Curve)")
    print("  â”‚       â”œâ”€â”€ precision_recall_curve.png (Precision-Recall Curve)")
    print("  â”‚       â””â”€â”€ metrics.json              (Metrics data)")
    print("  â”‚")
    print("  â”œâ”€â”€ 80_20/")
    print("  â”‚   â”œâ”€â”€ training/")
    print("  â”‚   â”‚   â””â”€â”€ training_curves.png")
    print("  â”‚   â””â”€â”€ metrics/")
    print("  â”‚       â”œâ”€â”€ confusion_matrix.png")
    print("  â”‚       â”œâ”€â”€ metrics_summary.png")
    print("  â”‚       â”œâ”€â”€ roc_auc_curve.png")
    print("  â”‚       â”œâ”€â”€ precision_recall_curve.png")
    print("  â”‚       â””â”€â”€ metrics.json")
    print("  â”‚")
    print("  â”œâ”€â”€ 80_10_10/")
    print("  â”‚   â”œâ”€â”€ training/")
    print("  â”‚   â”‚   â””â”€â”€ training_curves.png")
    print("  â”‚   â””â”€â”€ metrics/")
    print("  â”‚       â”œâ”€â”€ confusion_matrix.png")
    print("  â”‚       â”œâ”€â”€ metrics_summary.png")
    print("  â”‚       â”œâ”€â”€ roc_auc_curve.png")
    print("  â”‚       â”œâ”€â”€ precision_recall_curve.png")
    print("  â”‚       â””â”€â”€ metrics.json")
    print("  â”‚")
    print("  â”œâ”€â”€ 70_15_15/")
    print("  â”‚   â”œâ”€â”€ training/")
    print("  â”‚   â”‚   â””â”€â”€ training_curves.png")
    print("  â”‚   â””â”€â”€ metrics/")
    print("  â”‚       â”œâ”€â”€ confusion_matrix.png")
    print("  â”‚       â”œâ”€â”€ metrics_summary.png")
    print("  â”‚       â”œâ”€â”€ roc_auc_curve.png")
    print("  â”‚       â”œâ”€â”€ precision_recall_curve.png")
    print("  â”‚       â””â”€â”€ metrics.json")
    print("  â”‚")
    print("  â””â”€â”€ 60_20_20/")
    print("      â”œâ”€â”€ training/")
    print("      â”‚   â””â”€â”€ training_curves.png")
    print("      â””â”€â”€ metrics/")
    print("          â”œâ”€â”€ confusion_matrix.png")
    print("          â”œâ”€â”€ metrics_summary.png")
    print("          â”œâ”€â”€ roc_auc_curve.png")
    print("          â”œâ”€â”€ precision_recall_curve.png")
    print("          â””â”€â”€ metrics.json")
    
    print("\nðŸ“ˆ VISUALIZATIONS INCLUDED PER SPLIT:\n")
    print("  Training Curves (training_curves.png):")
    print("    âœ“ F1 Score vs Epochs")
    print("    âœ“ Accuracy vs Epochs")
    print("    âœ“ Precision vs Epochs")
    print("    âœ“ Recall vs Epochs")
    print("    âœ“ Train/Val Loss vs Epochs")
    print("    âœ“ Combined Performance")
    print("\n  Metrics Summary (metrics_summary.png):")
    print("    âœ“ Performance Metrics Bar Chart")
    print("    âœ“ Confusion Matrix Heatmap")
    print("    âœ“ Per-Class Detection Rate")
    print("    âœ“ Detailed Statistics")
    print("\n  Other Visualizations:")
    print("    âœ“ ROC-AUC Curve (roc_auc_curve.png)")
    print("    âœ“ Precision-Recall Curve (precision_recall_curve.png)")
    print("    âœ“ Confusion Matrix (confusion_matrix.png)")
    print("\n  Data Files:")
    print("    âœ“ Metrics JSON (metrics.json)")
    
    print("\n" + "=" * 80)
    print("  READY TO USE!")
    print("=" * 80 + "\n")
    
    print("Next steps:")
    print("  1. Check runs/detect/splits_metrics/ for all visualizations")
    print("  2. Open PNG files to view the performance metrics")
    print("  3. Compare metrics across different splits")
    print("  4. Choose the best split strategy based on performance")
    print("\n")


if __name__ == '__main__':
    main()
